{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Brainwaves to Pixels: Notes on EEG-Based Image Reconstruction","text":""},{"location":"index.html#about","title":"About","text":"<p>Hi~\ud83d\ude09 I\u2019m Ignisavium, a student passionate about EEG-based image reconstruction. These notes are part of my ongoing effort to organize and share knowledge on this fascinating topic. Feel free to explore my work or contribute on GitHub: @ignisavium . </p>"},{"location":"index.html#key-topics-covered","title":"Key Topics Covered","text":"<ul> <li>\ud83e\udde0Introduction: Basics of EEG and image reconstruction techniques. (EEG itself; Diffusion Model Intro</li> <li>\ud83d\udcd8Papers &amp; Methods: Insights from seminal studies and algorithms.</li> <li>\ud83e\udd14Reflections : Limitations and future directions in the field.</li> </ul> <p>In recent years, Diffusion Model has gradually become the mainstream of generative models due to its excellent performance and convenient integration features. Here we mainly focus on the EEG to Image reconstruction method based on diffusion.</p>"},{"location":"index.html#paper-compilation","title":"Paper Compilation","text":"<p>This is a summary of cutting-edge work in recent years.</p>"},{"location":"index.html#eeg-based-image-reconstruction","title":"EEG-based Image Reconstruction","text":"\u65f6\u95f4 \u6807\u9898 2210 Decoding Visual Neural Representations by Multimodal Learning of Brain-Visual-Linguistic Features 2211 High-resolution image reconstruction with latent diffusion models from human brain activity 2211 Seeing beyond the brain\uff1aConditional diffusion model with sparse masked modeling for vision decoding 2212 NeuroGAN\uff1aimage reconstruction from EEG signals via an attention-based GAN 2302 EEG2IMAGE\uff1aImage Reconstruction from EEG Brain Signals 2303 DCA\uff1aA dual conditional autoencoder framework for the reconstruction from EEG into image 2303 High-resolution image reconstruction with latent diffusion models from human brain activity 2303 MindDiffuser\uff1aControlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion 2303 [Apdx]High-resolution image reconstruction with latent diffusion models from human brain activity 2305 Reconstructing the Mind's Eye\uff1afMRI-to-Image with Contrastive Learning and Diffusion Priors 2306 DreamDiffusion\uff1aGenerating High-Quality Images from Brain EEG Signals 2306 Improving visual image reconstruction from human brain activity using latent diffusion models via multiple decoded inputs 2308 Decoding Natural Images from EEG for Object Recognition 2308 Seeing through the Brain\uff1aImage Reconstruction of Visual Perception from Human Brain Signals 2308 UniBrain\uff1aUnify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity 2309 Decoding visual brain representations from electroencephalography through Knowledge Distillation and latent diffusion models 2309 DM-RE2I\uff1aA framework based on diffusion model for the reconstruction from EEG to image 2310 DM-RE2I\uff1aA framework based on diffusion model for the reconstruction from EEG to image 2310 Learning Robust Deep Visual Representations from EEG Brain Recordings 2310 Learning_Robust_Deep_Visual_Representations_From_EEG_Brain_Recordings_WACV_2024_paper 2312 BrainVis\uff1aExploring the Bridge between Brain and Visual Signals via Image Reconstruction 2401 Visual image reconstruction based on EEG signals using a generative adversarial and deep fuzzy neural network 2402 MambaMIR\uff1aAn Arbitrary-Masked Mamba for Joint Medical Image Reconstruction and Uncertainty Estimation 2402 Next Generation Imaging in Consumer Technology for ERP Detection-Based EEG Cross-Subject Visual Object Recognition 2403 Reconstructing Visual Stimulus Representation From EEG Signals Based on Deep Visual Representation Model 2403 Semantics-Guided Hierarchical Feature Encoding Generative Adversarial Network for fMRI2img 2403 Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion 2403 Visual Decoding and Reconstruction via EEG 2404 A New Framework Combining Diffusion Models and the Convolution Classifier for Generating Images from EEG Signals 2404 Neuro-Vision to Language\uff1aEnhancing Brain Recording-based Visual Reconstruction and Language Interaction 2405 A New Framework Combining Diffusion Models and the Convolution Classifier for Generating Images from EEG Signals 2406 Autoregressive Model Beats Diffusion\uff1aLlama for Scalable Image Generation 2406 Evaluating the Feasibility of Visual Imagery for an EEG-Based Brain\u2013Computer Interface 2406 Evaluating the Feasibility of Visual Imagery for an EEG-Based Brain\u2013Computer Interfacepfd 2406 Mind's Eye\uff1aImage Recognition by EEG via Multimodal Similarity-Keeping Contrastive Learning 2407 EidetiCom\uff1aA Cross-modal Brain-Computer Semantic Communication Paradigm for Decoding Visual Perception 2407 Image classification and reconstruction from low-density EEG 2407 MB2C\uff1aMultimodal Bidirectional Cycle Consistency for Learning Robust Visual Neural Representations 2409 BrainDecoder\uff1aStyle-Based Visual Decoding of EEG Signals 2410 NECOMIMI\uff1aNeural-Cognitive Multimodal EEG-informed Image Generation with Diffusion Models 2410 Research on Brain Visual Image Signal Recognition Method Based on Deep Neural Network 2412 CognitionCapturer\uff1aDecoding Visual Stimuli From Human EEG Signal With Multimodal Information 2505 Deep Learning for EEG-Based Visual Classification and Reconstruction\uff1aPanorama, Trends, Challenges and Opportunities 2505 [Survey] Visual Image Reconstruction from Brain Activity via Latent Representation 2507 [Survey] Interpretable EEG-to-Image Generation with Semantic Prompts"},{"location":"index.html#diffusion-model","title":"Diffusion Model","text":"\u65f6\u95f4 \u6807\u9898 Tutorial on Diffusion Models for Imaging and Vision [DDPM]Denoising Diffusion Probabilistic Models [Score Matching]Score-Based Generative Modeling through SDEs [Stable Diffusion]High-Resolution Image Synthesis with Latent Diffusion Models 2105 Diffusion Models Beat GANs on Image Synthesis 2211 Versatile diffusion\uff1aText, images and variations all in one diffusion model 2308 IP-Adapter\uff1aText Compatible Image Prompt Adapter for Text-to-Image Diffusion Models 2310 Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation 2406 Autoregressive Model Beats Diffusion\uff1aLlama for Scalable Image Generation"},{"location":"index.html#_1","title":"Front page","text":""},{"location":"A%20New%20Framework%20Combining%20Diffusion%20and%20Convolution%20Classifier%20for%20Generating%20Images%20from%20EEG%20Signals.html","title":"A New Framework Combining Diffusion Models and the Convolution Classifier for Generating Images from EEG Signals","text":""},{"location":"A%20New%20Framework%20Combining%20Diffusion%20and%20Convolution%20Classifier%20for%20Generating%20Images%20from%20EEG%20Signals.html#info","title":"\ud83d\udd25INFO","text":"<p>Blog: 2025/07/31 by IgniSavium</p> <ul> <li>Title: A New Framework Combining Diffusion Models and the Convolution Classifier for Generating Images from EEG Signals</li> <li>Authors: Guangyu Yang and Jinguo Liu (CAS)</li> <li>Published: April 2024</li> <li>Comment: Brain Science</li> <li>URL: http://dx.doi.org/10.3390/brainsci14050478</li> </ul> <p>\ud83e\udd5cTLDR: EEG Conv-Encoder + PEFT SD.</p>"},{"location":"A%20New%20Framework%20Combining%20Diffusion%20and%20Convolution%20Classifier%20for%20Generating%20Images%20from%20EEG%20Signals.html#motivation","title":"Motivation","text":"<p>This paper aims to address the challenge of generating high-quality images from complex EEG signals\u2014characterized by low spatial resolution and noise\u2014by proposing a novel EEG-ConDiffusion framework that overcomes the limitations of prior EEG-to-image methods such as LSTM, GANs, and VAEs through effective CNN-based feature extraction and fine-tuning of a pretrained stable diffusion model.</p>"},{"location":"A%20New%20Framework%20Combining%20Diffusion%20and%20Convolution%20Classifier%20for%20Generating%20Images%20from%20EEG%20Signals.html#model","title":"Model","text":""},{"location":"A%20New%20Framework%20Combining%20Diffusion%20and%20Convolution%20Classifier%20for%20Generating%20Images%20from%20EEG%20Signals.html#architecture","title":"Architecture","text":"<ol> <li>Train EEG encoder by classification task.</li> <li>Train SD partially by reconstruction</li> </ol> <p>During the model fine-tuning process, we fix the remainder of the SD model and optimize the CLIP text encoder \\(\u03c4_\u03b8(y)\\), cross-attention head, and projection head at the same time...... EEG feature vectors that have undergone feature extraction and position encoding are used instead of text input to the pretrained CLIP Embedder. The CLIP Embedder was fine-tuned to help align the EEG feature vector space with the image feature space. Fine-tuning the cross-attention head is essential for bridging the pretraining conditional space and the latent space of the EEG features.</p> <p></p> <p>\ud83e\uddd0CLIP text embedder is very possibly NOT efficient here (big space gap between EEG Feature and Pure Text).</p> <p>EEG encoder utilizes temporal + spatial convolutions.</p> <p></p>"},{"location":"A%20New%20Framework%20Combining%20Diffusion%20and%20Convolution%20Classifier%20for%20Generating%20Images%20from%20EEG%20Signals.html#evaluation","title":"Evaluation","text":""},{"location":"A%20New%20Framework%20Combining%20Diffusion%20and%20Convolution%20Classifier%20for%20Generating%20Images%20from%20EEG%20Signals.html#frequency-band-influences","title":"frequency band influences","text":"<ol> <li>1-70Hz performs much better than 5-95Hz</li> <li>Subject Variance is very obvious for performance</li> </ol>"},{"location":"A%20New%20Framework%20Combining%20Diffusion%20and%20Convolution%20Classifier%20for%20Generating%20Images%20from%20EEG%20Signals.html#inter-subject-generalizability","title":"inter-subject generalizability","text":"<p>Use S1 weight as anchor.</p> <p></p> <p>\ud83e\uddd0Inception Score purely emphasizes the entropy of prediction scores, not very enough to show accuracy.</p>"},{"location":"BrainVis-Exploring%20the%20Bridge%20between%20Brain%20and%20Visual%20Signals%20via%20Image%20Reconstruction.html","title":"BrainVis: Exploring the Bridge between Brain and Visual Signals via Image Reconstruction","text":""},{"location":"BrainVis-Exploring%20the%20Bridge%20between%20Brain%20and%20Visual%20Signals%20via%20Image%20Reconstruction.html#info","title":"\ud83d\udd25INFO","text":"<p>Blog: 2025/07/23 by IgniSavium</p> <ul> <li>Title: BrainVis: Exploring the Bridge between Brain and Visual Signals via Image Reconstruction</li> <li>Authors:  Honghao Fu, Hao Wang et.al (HKUST-Guangzhou)</li> <li>Published: December 2023</li> <li>Comment: BrainVis: Exploring the Bridge between Brain and Visual Signals via Image Reconstruction</li> <li>URL: https://arxiv.org/abs/2312.14871</li> </ul> <p>\ud83e\udd5cTLDR: This paper introduces BrainVis, which reconstructs semantically accurate images from EEG by enhancing representations with (large scale) self-supervised learning and CLIP-based alignment.</p>"},{"location":"BrainVis-Exploring%20the%20Bridge%20between%20Brain%20and%20Visual%20Signals%20via%20Image%20Reconstruction.html#motivation","title":"Motivation","text":"<p>This paper is motivated by the challenge of reconstructing semantically accurate images from noisy EEG signals\u2014addressing limitations of previous methods (i.e. Dream Diffusion) such as weak EEG feature embedding, reliance on large (self-supervision) datasets, and inability to capture fine-grained semantics\u2014by proposing BrainVis, which enhances EEG representation through self-supervised learning (latent masked modeling) and frequency features, and improves cross-modal alignment with CLIP-based semantic interpolation, achieving superior results with significantly less training data.</p> <p></p>"},{"location":"BrainVis-Exploring%20the%20Bridge%20between%20Brain%20and%20Visual%20Signals%20via%20Image%20Reconstruction.html#model","title":"Model","text":""},{"location":"BrainVis-Exploring%20the%20Bridge%20between%20Brain%20and%20Visual%20Signals%20via%20Image%20Reconstruction.html#architecture","title":"Architecture","text":"<p>The main target is to map EEG to conditional text input embeddings in Stable Diffusion.</p>"},{"location":"BrainVis-Exploring%20the%20Bridge%20between%20Brain%20and%20Visual%20Signals%20via%20Image%20Reconstruction.html#pretraining","title":"Pretraining","text":""},{"location":"BrainVis-Exploring%20the%20Bridge%20between%20Brain%20and%20Visual%20Signals%20via%20Image%20Reconstruction.html#time-branch","title":"Time Branch","text":"<p>The Latent Masked Modeling (LMM) pre-training method enhances EEG time-domain feature learning by dividing the signal \\(x \\in \\mathbb{R}^{c \\times l}\\) into n units, projecting each into d-dimensional embeddings \\(z \\in \\mathbb{R}^{n \\times d}\\), and applying random masking (ratio \\(r_m\\)) for self-supervised learning; the model optimizes two objectives: (1) regression loss \\(L_{\\text{reg}} = \\frac{1}{d} \\| f_m - f_{mp} \\|^2_2\\) to reconstruct masked embeddings using transformer-based predictions, and (2) classification loss \\(L_{\\text{cls}} = -\\mathbf{l}_m \\cdot \\log(\\mathbf{p}_m)\\) via codebook tokenization of masked units, with the total loss defined as \\(L_{\\text{lmm}} = L_{\\text{reg}} + L_{\\text{cls}}\\).</p> <p>(channel = 128, time_step = 440, n = 110, d = 1024, \\(r_M\\) = 0.75 and \\(n_{\\text{code}}\\) = 660)</p> <p>\u2728related work: Momentum Encoder ; Vector Quantized VAE</p>"},{"location":"BrainVis-Exploring%20the%20Bridge%20between%20Brain%20and%20Visual%20Signals%20via%20Image%20Reconstruction.html#frequency-branch","title":"Frequency Branch","text":"<ol> <li>Frequency Transformation: EEG signals are converted to the frequency domain using Fast Fourier Transform (FFT).</li> <li>Feature Extraction: An LSTM model is used to extract frequency features, avoiding overfitting risks of complex networks.</li> <li>Supervised Training: The LSTM is trained with visual classification labels using cross-entropy (CE) loss.</li> </ol> <p>Unified Classify: The time and frequency branches are fine-tuned together using CE loss to form a unified time-frequency embedding.</p>"},{"location":"BrainVis-Exploring%20the%20Bridge%20between%20Brain%20and%20Visual%20Signals%20via%20Image%20Reconstruction.html#clip-alignment-finetuing","title":"CLIP alignment finetuing","text":"<p>Find a balance (simple sum of loss) between label-induced coarse text feature and description-induced fine text feature. (\ud83e\uddd0Possibly tuned together with Unified Classify)</p> <p></p>"},{"location":"BrainVis-Exploring%20the%20Bridge%20between%20Brain%20and%20Visual%20Signals%20via%20Image%20Reconstruction.html#refined-sd-generation","title":"Refined SD Generation","text":"<p>Img2Img Refinement using EEG classification labels (inferred from Unified Classify) \uff08using only ''label word\" as refinement conditional text\uff09 to enhance image quality.</p>"},{"location":"BrainVis-Exploring%20the%20Bridge%20between%20Brain%20and%20Visual%20Signals%20via%20Image%20Reconstruction.html#evaluation","title":"Evaluation","text":""},{"location":"BrainVis-Exploring%20the%20Bridge%20between%20Brain%20and%20Visual%20Signals%20via%20Image%20Reconstruction.html#performance","title":"Performance","text":""},{"location":"BrainVis-Exploring%20the%20Bridge%20between%20Brain%20and%20Visual%20Signals%20via%20Image%20Reconstruction.html#ablation","title":"Ablation","text":"<p>\ud83e\uddd0Time Branch is the main information source vs. Frequency Branch.</p> <p></p> <p>\ud83e\uddd0Seems that refinement (single-label guided image2image SD refinement) dominates the output semantics, thus original image structure (size, position, orientation, action etc.) will be largely ignored.</p> <p></p> <p></p>"},{"location":"BrainVis-Exploring%20the%20Bridge%20between%20Brain%20and%20Visual%20Signals%20via%20Image%20Reconstruction.html#reflections","title":"\ud83e\uddd0Reflections","text":"<ul> <li> <p>There's no experiment to show the effectiveness of MASKED LATENT MODELING classification objective (codebook design).</p> </li> <li> <p>Seems that even simple object class recognition is still a little bit hard for EEG-signal analysis\uff08acc. ~45%\uff09 compared with fMRI modeling\uff08acc. ~75%\uff09 .</p> </li> </ul> <p></p>"},{"location":"DM-RE2I-A%20framework%20based%20on%20diffusion%20model%20for%20the%20reconstruction%20from%20EEG%20to%20image.html","title":"DM-RE2I: A framework based on diffusion model for the reconstruction from EEG to image","text":""},{"location":"DM-RE2I-A%20framework%20based%20on%20diffusion%20model%20for%20the%20reconstruction%20from%20EEG%20to%20image.html#info","title":"\ud83d\udd25INFO","text":"<p>Blog: 2025/07/19 by IgniSavium</p> <ul> <li>Title: DM-RE2I: A framework based on diffusion model for the reconstruction from EEG to image</li> <li>Authors:  Hong Zeng, Wanzeng Kong et.al (Hangzhou Dianzi University, University of Yamanashi)</li> <li>Published: October 2023</li> <li>Comment: Biomedical Signal Processing and Control</li> <li>URL: https://www.sciencedirect.com/science/article/abs/pii/S174680942300558X</li> </ul> <p>\ud83e\udd5cTLDR: TSConv + guided DDPM</p>"},{"location":"DM-RE2I-A%20framework%20based%20on%20diffusion%20model%20for%20the%20reconstruction%20from%20EEG%20to%20image.html#motivation","title":"Motivation","text":"<p>The paper aims to address the challenge of reconstructing high-quality, semantically accurate images from EEG signals\u2014which suffer from low signal-to-noise ratio and individual variability\u2014by proposing a novel diffusion-model-based framework (DM-RE21) that combines a robust EEG semantic feature extractor (EVRNet - one residual net) and a denoising diffusion module (EG-DDPM), overcoming the limitations of previous LSTM, CNN, VAE, and GAN-based approaches in terms of semantic fidelity, resolution, and generalizability.</p>"},{"location":"DM-RE2I-A%20framework%20based%20on%20diffusion%20model%20for%20the%20reconstruction%20from%20EEG%20to%20image.html#model","title":"Model","text":""},{"location":"DM-RE2I-A%20framework%20based%20on%20diffusion%20model%20for%20the%20reconstruction%20from%20EEG%20to%20image.html#eeg-encoder","title":"EEG Encoder","text":""},{"location":"DM-RE2I-A%20framework%20based%20on%20diffusion%20model%20for%20the%20reconstruction%20from%20EEG%20to%20image.html#image-decoder","title":"Image Decoder","text":""},{"location":"Decoding%20visual%20brain%20representations%20from%20EEG%20through%20Knowledge%20Distillation%20and%20latent%20diffusion.html","title":"Decoding visual brain representations from electroencephalography through Knowledge Distillation and latent diffusion models","text":""},{"location":"Decoding%20visual%20brain%20representations%20from%20EEG%20through%20Knowledge%20Distillation%20and%20latent%20diffusion.html#info","title":"\ud83d\udd25INFO","text":"<p>Blog: 2025/07/23 by IgniSavium</p> <ul> <li>Title: Decoding visual brain representations from electroencephalography through   Knowledge Distillation and latent diffusion models</li> <li>Authors: Matteo Ferrante, Nicola Toschi (University of Rome Tor Vergata)</li> <li>Published: September 2023</li> <li>Comment: arxiv</li> <li>URL: https://arxiv.org/abs/2309.07149</li> </ul> <p>\ud83e\udd5cTLDR: Only train the Conv Encoder which maps EEG-signal spectrogram (derived from STFT) to ImageNet classification scores with CLIP knowledge distillation], and then use a category-related text template to guide Stable Diffusion generation.</p>"},{"location":"Decoding%20visual%20brain%20representations%20from%20EEG%20through%20Knowledge%20Distillation%20and%20latent%20diffusion.html#motivation","title":"Motivation","text":"<p>This research aims to enhance EEG-based brain decoding by developing an individualized, real-time image classification and reconstruction pipeline, addressing the limitations of prior studies that relied on multisubject models and struggled with low-fidelity visual reconstructions.</p>"},{"location":"Decoding%20visual%20brain%20representations%20from%20EEG%20through%20Knowledge%20Distillation%20and%20latent%20diffusion.html#model","title":"Model","text":""},{"location":"Decoding%20visual%20brain%20representations%20from%20EEG%20through%20Knowledge%20Distillation%20and%20latent%20diffusion.html#architecture","title":"Architecture","text":"<ol> <li>train a EEG-based classifier using CLIP distillation:</li> </ol> <ol> <li>Use text prompt such as \"an image of a \\&lt;predicted class&gt;\" to guide SD generation with random white noise \\(z_T\\).</li> </ol>"},{"location":"Decoding%20visual%20brain%20representations%20from%20EEG%20through%20Knowledge%20Distillation%20and%20latent%20diffusion.html#evaluation","title":"Evaluation","text":""},{"location":"Decoding%20visual%20brain%20representations%20from%20EEG%20through%20Knowledge%20Distillation%20and%20latent%20diffusion.html#performance","title":"Performance","text":"<p>Compared with other typical CLASSIFIER architectures.</p> <p>\ud83e\udd14Last two rows in the table below show the benefit of knowledge distillation, but the acc. gain is VERY LIKELY exaggerated.</p> <p>\ud83e\udd14Using typical LSTM to model EEG time series directly can also have not bad performance.</p> <p></p> <p>This bar chart below seems more plausible.</p> <p></p>"},{"location":"Decoding%20visual%20brain%20representations%20from%20EEG%20through%20Knowledge%20Distillation%20and%20latent%20diffusion.html#reflections","title":"\ud83e\udd14Reflections","text":"<p>Class-wise encoding is obviously insufficient for detailed semantic reconstruction.</p> <p>Other works have mapped the EEG-signal to conditional text features and visual latent features in the SD.</p>"},{"location":"DreamDiffusion-Generating%20High-Quality%20Images%20from%20Brain%20EEG%20Signals.html","title":"DreamDiffusion: Generating High-Quality Images from Brain EEG Signals","text":""},{"location":"DreamDiffusion-Generating%20High-Quality%20Images%20from%20Brain%20EEG%20Signals.html#info","title":"\ud83d\udd25INFO","text":"<p>Blog: 2025/07/22 by IgniSavium</p> <ul> <li>Title: DreamDiffusion: Generating High-Quality Images from Brain EEG Signals</li> <li>Authors: Yunpeng Bai, Chun Yuan (THU Shenzhen Inernational)</li> <li>Published: June 2023</li> <li>Comment: ECCV</li> <li>URL: https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04605.pdf</li> </ul> <p>\ud83e\udd5cTLDR: Highlight is a MAE (very large scale) self-supervised learning method for EEG Encoder.</p>"},{"location":"DreamDiffusion-Generating%20High-Quality%20Images%20from%20Brain%20EEG%20Signals.html#motivation","title":"Motivation","text":"<p>This research aims to enable direct, high-quality image generation from EEG signals\u2014overcoming the limitations of prior work that relied on costly, non-portable fMRI or poorly aligned EEG-text-image mappings\u2014by leveraging pre-trained diffusion models and novel temporal EEG pretraining (masked signal modeling) to realize low-cost, accessible \u201cthoughts-to-image\u201d applications.</p> <p></p>"},{"location":"DreamDiffusion-Generating%20High-Quality%20Images%20from%20Brain%20EEG%20Signals.html#model","title":"Model","text":""},{"location":"DreamDiffusion-Generating%20High-Quality%20Images%20from%20Brain%20EEG%20Signals.html#architecture","title":"Architecture","text":""},{"location":"DreamDiffusion-Generating%20High-Quality%20Images%20from%20Brain%20EEG%20Signals.html#self-supervised-learning-for-eeg-modeling","title":"self-supervised learning for EEG modeling","text":"<p>Given the high temporal resolution of EEG signals, we first divide them into tokens in the time domain, and randomly mask a certain percentage of tokens. Subsequently, these tokens will be transformed into embeddings by using a one dimensional convolutional layer. Then, we use an asymmetric architecture such as MAE  to predict the missing tokens based on contextual cues from the surrounding tokens.</p> <p>reconstruction loss is only applied at masked token and unified across all EEG channels.</p>"},{"location":"DreamDiffusion-Generating%20High-Quality%20Images%20from%20Brain%20EEG%20Signals.html#fine-tuning-with-stable-diffusion-framework","title":"fine-tuning with Stable Diffusion Framework","text":"<p>only update the encoder(+projector) and the cross_attn in U-Net (denoise network in SD) with combined loss:</p> <ol> <li>original image reconstruction loss    </li> <li>align projected feature to CLIP description feature    </li> </ol> <p>The pre-trained Stable Diffusion model is specifically trained for text-to-image generation; however, the EEG signal has its own characteristics, and its latent space is quite different from that of text and image. Therefore, directly fine-tuning the Stable Diffusion model using limited EEG-image paired data is unlikely to accurately align the EEG features with the text embeddings (\ud83e\uddd0CLIP embeddings in SD).</p>"},{"location":"DreamDiffusion-Generating%20High-Quality%20Images%20from%20Brain%20EEG%20Signals.html#data","title":"Data","text":"<p>Due to variations in the equipment used for data acquisition, the channel counts of these EEG data samples differ significantly. To facilitate pre-training, we have uniformly padded all the data that has fewer channels to 128 channels by filling missing channels with replicated values. </p>"},{"location":"DreamDiffusion-Generating%20High-Quality%20Images%20from%20Brain%20EEG%20Signals.html#evaluation","title":"Evaluation","text":""},{"location":"DreamDiffusion-Generating%20High-Quality%20Images%20from%20Brain%20EEG%20Signals.html#performance","title":"Performance","text":""},{"location":"DreamDiffusion-Generating%20High-Quality%20Images%20from%20Brain%20EEG%20Signals.html#1-fid-frechet-inception-distance","title":"1. FID (Fr\u00e9chet Inception Distance)","text":"<p>Purpose: Measures the difference in feature space distribution between generated and real images. Formula: $$ \\mathrm{FID} = |\\mu_r - \\mu_g|_2^2 + \\mathrm{Tr}\\left( \\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{1/2} \\right) $$</p> <ul> <li>\\(\\mu_r, \\Sigma_r\\): Mean and covariance of real image features. </li> <li>\\(\\mu_g, \\Sigma_g\\): Mean and covariance of generated image features.  </li> <li>Features are typically extracted from a layer of the Inception-V3 network.  </li> </ul>"},{"location":"DreamDiffusion-Generating%20High-Quality%20Images%20from%20Brain%20EEG%20Signals.html#2-psnr-peak-signal-to-noise-ratio","title":"2. PSNR (Peak Signal-to-Noise Ratio)","text":"<p>Purpose: Evaluates pixel-level error in image reconstruction/compression. Formula:  </p> \\[ \\mathrm{PSNR} = 10 \\cdot \\log_{10} \\left( \\frac{MAX_I^2}{\\mathrm{MSE}} \\right) \\] <ul> <li>\\(MAX_I\\): Maximum pixel value of the image (usually 255).  </li> <li>\\(\\mathrm{MSE}\\): Mean Squared Error, calculated as:  </li> </ul> \\[ \\mathrm{MSE} = \\frac{1}{MN} \\sum_{i=1}^{M} \\sum_{j=1}^{N} \\left[I(i,j) - K(i,j)\\right]^2 \\]"},{"location":"DreamDiffusion-Generating%20High-Quality%20Images%20from%20Brain%20EEG%20Signals.html#3-lpips-learned-perceptual-image-patch-similarity","title":"3. LPIPS (Learned Perceptual Image Patch Similarity)","text":"<p>Purpose: Evaluates perceptual differences between images (aligned with human vision). Formula (based on deep features):  </p> \\[ \\mathrm{LPIPS}(x, x_0) = \\sum_l \\frac{1}{H_l W_l} \\sum_{h,w} w_l \\left\\| \\hat{y}^l_{h,w} - y^l_{h,w} \\right\\|_2^2 \\] <ul> <li>\\(y^l, \\hat{y}^l\\): Feature maps extracted from layer \\(l\\) (normalized).  </li> <li>\\(w_l\\): Learned weights for each layer.  </li> <li>Measures differences in deep feature representations, not pixel-level.</li> </ul> <p></p> <p></p>"},{"location":"DreamDiffusion-Generating%20High-Quality%20Images%20from%20Brain%20EEG%20Signals.html#ablation","title":"Ablation","text":"<p>\ud83e\uddd0Only tuning the pretrained encoder with CLIP text supervision yields obvious worse acc. compared with both tuning the encoder and the cross_attn in U-Net. Maybe it's because this framework doesn't map the EEG feature to the original image's latent state (z).</p> <p></p> <p></p>"},{"location":"DreamDiffusion-Generating%20High-Quality%20Images%20from%20Brain%20EEG%20Signals.html#reflection","title":"\ud83e\uddd0Reflection","text":"<p>We can try implement CLIP alignment first, and then implement image reconstruction tuning later. i.e. divide this combined loss into two consequent process.</p>"},{"location":"High-resolution%20image%20reconstruction%20with%20latent%20diffusion%20models%20from%20human%20brain%20activity.html","title":"High-resolution image reconstruction with latent diffusion models from human brain activity","text":""},{"location":"High-resolution%20image%20reconstruction%20with%20latent%20diffusion%20models%20from%20human%20brain%20activity.html#info","title":"\ud83d\udd25INFO","text":"<p>Blog: 2025/07/21 by IgniSavium</p> <ul> <li>Title: High-resolution image reconstruction with latent diffusion models from human brain activity</li> <li>Authors: Yu Takagi, Shinji Nishimoto (Osaka University)</li> <li>Published: March 2023</li> <li>Comment: CVPR</li> <li>URL: https://www.biorxiv.org/content/10.1101/2022.11.18.517004v3</li> </ul> <p>\ud83e\udd5cTLDR: Image reconstruction via training-free latent diffusion models ( Stable Diffusion) with fMRI inputs.</p>"},{"location":"High-resolution%20image%20reconstruction%20with%20latent%20diffusion%20models%20from%20human%20brain%20activity.html#motivation","title":"Motivation","text":"<p>This research aims to improve the reconstruction of high-resolution, semantically accurate images from human brain activity (fMRI) using training-free latent diffusion models (LDMs), addressing the limitations of previous deep generative models that required complex training and fine-tuning, by offering a more efficient and interpretable method with minimal computational cost.</p>"},{"location":"High-resolution%20image%20reconstruction%20with%20latent%20diffusion%20models%20from%20human%20brain%20activity.html#model","title":"Model","text":""},{"location":"High-resolution%20image%20reconstruction%20with%20latent%20diffusion%20models%20from%20human%20brain%20activity.html#architecture","title":"Architecture","text":"<p>Overall a Latent Diffusion Model framework (actually <code>Stable Diffusion</code> ):  $ \\epsilon $ denotes an image encoder, $ D $ is a image decoder (they are a pair of autoencoder). </p> <p></p> <p>only train 2 (small linear) parts:</p> <p>(1) fMRI -&gt; original image latent \\(z\\)  (a simple coarse network approximation)</p> <p>(2) fMRI -&gt; conditional \"text\" (i.e. semantic) \\(c\\)</p>"},{"location":"High-resolution%20image%20reconstruction%20with%20latent%20diffusion%20models%20from%20human%20brain%20activity.html#evaluation","title":"Evaluation","text":"<p>No comparable previous work exists, so this paper almost only provides ablation and visualization analysis.</p>"},{"location":"High-resolution%20image%20reconstruction%20with%20latent%20diffusion%20models%20from%20human%20brain%20activity.html#ablation","title":"Ablation","text":"<p>Perceptual Similarity Metrics (PSMs):</p> <p></p> <p></p> <p>\\(z\\) represents the low-level semantics, and \\(c\\) captures the high-level abstraction semantics.</p>"},{"location":"High-resolution%20image%20reconstruction%20with%20latent%20diffusion%20models%20from%20human%20brain%20activity.html#reverse-encoding-visualization","title":"Reverse Encoding Visualization","text":"<p>Reversely map the features or components in the LDM BACK INTO the fMRI inputs (to find semantic relationships).</p>"},{"location":"High-resolution%20image%20reconstruction%20with%20latent%20diffusion%20models%20from%20human%20brain%20activity.html#zcz_c-feature","title":"\\(z,c,z_c\\) feature","text":"<p>Different latent representations (z, c, and \\(z_c\\)) show varying prediction performance across visual cortex regions, with z performing well in early visual cortex, c excelling in higher visual cortex, and \\(z_c\\) closely resembling z in its performance, despite representing visually different images</p>"},{"location":"High-resolution%20image%20reconstruction%20with%20latent%20diffusion%20models%20from%20human%20brain%20activity.html#zc-semantic-balancing-with-varying-noise-level","title":"\\(z,c\\)  semantic balancing with varying noise level","text":"<p>As noise levels increased, the latent representation with added noise (\\(z_c\\)) predicted voxel activity in higher visual cortex better (\ud83e\udd14thanks to high-level condition \\(c\\)) than the original representation (\\(z\\))</p>"},{"location":"High-resolution%20image%20reconstruction%20with%20latent%20diffusion%20models%20from%20human%20brain%20activity.html#z-c-semantic-balancing-at-different-denoising-timesteps","title":"\\(z, c\\) semantic balancing at different denoising timesteps","text":"<p>During the denoising process, early stages were dominated by the original image representation (z), while mid-stages saw the noise-added representation (\\(z_c\\)) ( \ud83e\udd14thanks to high-level condition \\(c\\)) better predict activity in higher visual cortex.</p>"},{"location":"High-resolution%20image%20reconstruction%20with%20latent%20diffusion%20models%20from%20human%20brain%20activity.html#denoise-network-per-layer-semantic-interpretation","title":"denoise network per-layer semantic interpretation","text":"<p>The U-Net bottleneck layer initially captures the most information across the cortex, but as denoising progresses, early U-Net layers become more predictive of early visual cortex activity while the bottleneck layer shifts to representing higher-level semantic information in higher visual areas.</p>"},{"location":"Learning%20Robust%20Deep%20Visual%20Representations%20from%20EEG%20Brain%20Recordings.html","title":"Learning Robust Deep Visual Representations from EEG Brain Recordings","text":""},{"location":"Learning%20Robust%20Deep%20Visual%20Representations%20from%20EEG%20Brain%20Recordings.html#info","title":"\ud83d\udd25INFO","text":"<p>Blog: 2025/07/28 by IgniSavium</p> <ul> <li>Title: Learning Robust Deep Visual Representations from EEG Brain Recordings</li> <li>Authors: Prajwal Singh, Shanmuganathan Raman (IIT Gandhinagar, India)</li> <li>Published: October 2023</li> <li>Comment: WACV</li> <li>URL: https://openaccess.thecvf.com/content/WACV2024/papers/Singh_Learning_Robust_Deep_Visual_Representations_From_EEG_Brain_Recordings_WACV_2024_paper.pdf</li> </ul> <p>\ud83e\udd5cTLDR: Encoder Training: additional CLIP distillation</p>"},{"location":"Learning%20Robust%20Deep%20Visual%20Representations%20from%20EEG%20Brain%20Recordings.html#motivation","title":"Motivation","text":"<p>This paper aims to overcome the limitations of low-quality image synthesis and heavy reliance on label supervision in EEG-based image generation by proposing two-stage framework (EEGStyleGAN-ADA) that significantly improves synthesis quality and generalizability across datasets compared to prior state-of-the-art methods.</p>"},{"location":"Learning%20Robust%20Deep%20Visual%20Representations%20from%20EEG%20Brain%20Recordings.html#model","title":"Model","text":""},{"location":"Learning%20Robust%20Deep%20Visual%20Representations%20from%20EEG%20Brain%20Recordings.html#architecture","title":"Architecture","text":"<ol> <li>Train the EEG feature encoder by triplets loss.</li> </ol> <ol> <li>Fine-tune the EEG feature encoder by standard CLIP loss.</li> </ol> <ol> <li>Use the CLIP-aligned EEG feature as StyleGAN-ADA input.</li> </ol>"},{"location":"Learning%20Robust%20Deep%20Visual%20Representations%20from%20EEG%20Brain%20Recordings.html#evaluation","title":"Evaluation","text":""},{"location":"Learning%20Robust%20Deep%20Visual%20Representations%20from%20EEG%20Brain%20Recordings.html#pre-training-effectiveness","title":"Pre-training Effectiveness","text":"<p>triplet loss vs. supervised classification loss linear separability</p> <p></p> <p></p> <p></p> <p></p>"},{"location":"Learning%20Robust%20Deep%20Visual%20Representations%20from%20EEG%20Brain%20Recordings.html#image-synthesis-performance","title":"Image Synthesis Performance","text":""},{"location":"Learning%20Robust%20Deep%20Visual%20Representations%20from%20EEG%20Brain%20Recordings.html#data-simulation","title":"Data simulation","text":""},{"location":"Learning%20Robust%20Deep%20Visual%20Representations%20from%20EEG%20Brain%20Recordings.html#image-retrieval-validation","title":"Image Retrieval Validation","text":""},{"location":"MIND%27S%20EYE-IMAGE%20RECOGNITION%20BY%20EEG%20VIA%20MULTIMODAL%20SIMILARITY-KEEPING%20CONTRASTIVE%20LEARNING.html","title":"MIND'S EYE: IMAGE RECOGNITION BY EEG VIA MULTIMODAL SIMILARITY-KEEPING CONTRASTIVE LEARNING","text":""},{"location":"MIND%27S%20EYE-IMAGE%20RECOGNITION%20BY%20EEG%20VIA%20MULTIMODAL%20SIMILARITY-KEEPING%20CONTRASTIVE%20LEARNING.html#info","title":"\ud83d\udd25INFO","text":"<p>Blog: 2025/07/31 by IgniSavium</p> <ul> <li>Title: MIND'S EYE: IMAGE RECOGNITION BY EEG VIA MULTIMODAL SIMILARITY-KEEPING CONTRASTIVE LEARNING</li> <li>Authors: Chi-Sheng Chen, Chun-Shu Wei (National Yang Ming Chiao Tung University)</li> <li>Published: June 2024</li> <li>Comment: arxiv</li> <li>URL: https://arxiv.org/abs/2406.16910</li> </ul> <p>\ud83e\udd5cTLDR: Regularize contrastive loss with similarity-keeping term</p>"},{"location":"MIND%27S%20EYE-IMAGE%20RECOGNITION%20BY%20EEG%20VIA%20MULTIMODAL%20SIMILARITY-KEEPING%20CONTRASTIVE%20LEARNING.html#motivation","title":"Motivation","text":"<p>This paper aims to overcome the challenges of low signal-to-noise ratio and nonstationarity in EEG-based image decoding by proposing a self-supervised contrastive learning framework (MUSE) (similarity-keeping regularization term) with tailored EEG encoders, significantly outperforming prior methods in zero-shot image classification on large-scale datasets.</p>"},{"location":"MIND%27S%20EYE-IMAGE%20RECOGNITION%20BY%20EEG%20VIA%20MULTIMODAL%20SIMILARITY-KEEPING%20CONTRASTIVE%20LEARNING.html#model","title":"Model","text":"<p>EEG encoder architecture: (\ud83d\udd25upstream spatial conv + Spatial-Then-Time/Time-Then-Spatial combination + Graph Attention)</p> <p></p> <p>regularize the standard contrastive loss with Similarity Keeping term:</p> <p>\ud83e\udd14Innate character: Similarity Keeping term keeps the distribution of similarities with negative samples</p> <p></p>"},{"location":"MIND%27S%20EYE-IMAGE%20RECOGNITION%20BY%20EEG%20VIA%20MULTIMODAL%20SIMILARITY-KEEPING%20CONTRASTIVE%20LEARNING.html#evaluation","title":"Evaluation","text":"<p>SK term is helpful in Nerv-series architecture but not very helpful in MUSE-series.</p> <p></p>"},{"location":"MIND%27S%20EYE-IMAGE%20RECOGNITION%20BY%20EEG%20VIA%20MULTIMODAL%20SIMILARITY-KEEPING%20CONTRASTIVE%20LEARNING.html#interpretability","title":"Interpretability","text":"<p>MUSE-SK model highlights the model\u2019s enhanced focus on temporal and occipital areas\ud83e\uddd0Seems not obvious...</p> <p></p> <p></p>"},{"location":"MindDiffuser-Controlled%20Image%20Reconstruction%20from%20Human%20Brain%20Activity%20with%20Structural%20Diffusion.html","title":"MindDiffuser-Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion","text":""},{"location":"MindDiffuser-Controlled%20Image%20Reconstruction%20from%20Human%20Brain%20Activity%20with%20Structural%20Diffusion.html#info","title":"\ud83d\udd25INFO","text":"<p>Blog: 2025/07/21 by IgniSavium</p> <ul> <li>Title: MindDiffuser-Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion</li> <li>Authors: Yizhuo Lu, Huiguang He et al. (CAS)</li> <li>Published: March 2023</li> <li>Comment: ACM Multimedia</li> <li>URL: https://arxiv.org/abs/2303.14139</li> </ul> <p>\ud83e\udd5cTLDR: Preserve structural information in fMRI-to-image reconstruction by iteratively backpropagating Stable Diffusion inputs guided by CLIP low-layer feature supervision.</p>"},{"location":"MindDiffuser-Controlled%20Image%20Reconstruction%20from%20Human%20Brain%20Activity%20with%20Structural%20Diffusion.html#motivation","title":"Motivation","text":"<p>The research aims to address the challenge of reconstructing visual stimuli from fMRI data by developing a two-stage model (based on Stable Diffusion)  that simultaneously ensures semantic richness and structural alignment, overcoming limitations in previous methods that either lacked clear semantic details or controlled structural features like position and orientation.</p> <p></p>"},{"location":"MindDiffuser-Controlled%20Image%20Reconstruction%20from%20Human%20Brain%20Activity%20with%20Structural%20Diffusion.html#model","title":"Model","text":""},{"location":"MindDiffuser-Controlled%20Image%20Reconstruction%20from%20Human%20Brain%20Activity%20with%20Structural%20Diffusion.html#architecture","title":"Architecture","text":"<p>Stage-1: linear transform fMRI into original visual latent state \\(z\\) and text condition \\(c\\) in Stable Diffusion img2img process.</p> <p>Stage-2: iteratively backpropagate \\(z,c\\) to align the lower CLIP layer features (which contains structural information such as position and orientation) between generated images and original images (extracted via trained linear transformation from fMRI)</p>"},{"location":"MindDiffuser-Controlled%20Image%20Reconstruction%20from%20Human%20Brain%20Activity%20with%20Structural%20Diffusion.html#evaluation","title":"Evaluation","text":""},{"location":"MindDiffuser-Controlled%20Image%20Reconstruction%20from%20Human%20Brain%20Activity%20with%20Structural%20Diffusion.html#performance","title":"Performance","text":"<ul> <li>CLIP last layer cosine similarity for semantic comparison</li> </ul>"},{"location":"MindDiffuser-Controlled%20Image%20Reconstruction%20from%20Human%20Brain%20Activity%20with%20Structural%20Diffusion.html#ssim-structural-similarity-index-measure","title":"SSIM (Structural Similarity Index Measure)","text":"<ul> <li>From -1 to 1, but typically between 0 and 1;</li> <li>1 indicates that the two images are structurally identical.</li> </ul> \\[ SSIM(x,y)=\\frac{(2\\mu_x\\mu_y + C_1)(2\\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)} \\] <p>Where:</p> <ul> <li>\\(\\mu_x, \\mu_y\\): The mean values (brightness) of images \\(x\\) and \\(y\\);</li> <li>\\(\\sigma_x^2, \\sigma_y^2\\): Variance (contrast);</li> <li>\\(\\sigma_{xy}\\): Covariance (structural similarity);</li> <li>\\(C_1, C_2\\): Stabilizing constants to prevent division by zero.</li> </ul>"},{"location":"MindDiffuser-Controlled%20Image%20Reconstruction%20from%20Human%20Brain%20Activity%20with%20Structural%20Diffusion.html#per-pixel-correlation-coefficient-pixel-wise-correlation-coefficient","title":"Per-pixel correlation coefficient (Pixel-wise Correlation Coefficient)","text":"<ul> <li>From -1 to 1;</li> <li>1 indicates perfect positive correlation (every pixel matches);</li> <li>0 indicates no correlation;</li> <li>-1 indicates perfect negative correlation.</li> </ul> <p>Assuming the two images are flattened into two vectors \\(x\\) and \\(y\\), the correlation coefficient is: $$ r= \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}} $$ Where:</p> <ul> <li>\\(\\bar{x}, \\bar{y}\\) are the mean pixel values of the two images;</li> <li>This is essentially the Pearson correlation coefficient from statistics, applied to the pixel arrays of the images.</li> </ul> <p></p>"},{"location":"MindDiffuser-Controlled%20Image%20Reconstruction%20from%20Human%20Brain%20Activity%20with%20Structural%20Diffusion.html#_1","title":"2303.MindDiffuser-Controlled Image Reconstruction from Human Brain Activity with Structural Diffusion","text":""},{"location":"MindDiffuser-Controlled%20Image%20Reconstruction%20from%20Human%20Brain%20Activity%20with%20Structural%20Diffusion.html#cross-subject-adaptability","title":"Cross-subject adaptability","text":""},{"location":"MindDiffuser-Controlled%20Image%20Reconstruction%20from%20Human%20Brain%20Activity%20with%20Structural%20Diffusion.html#criticality-of-guided-origin-latent-z","title":"Criticality of guided origin latent \\(z\\)","text":""},{"location":"Reconstructing%20Visual%20Stimulus%20Images%20from%20EEG%20Signals%20Based%20on%20Deep%20Visual%20Representation%20Model.html","title":"Reconstructing Visual Stimulus Images from EEG Signals Based on Deep Visual Representation Model","text":""},{"location":"Reconstructing%20Visual%20Stimulus%20Images%20from%20EEG%20Signals%20Based%20on%20Deep%20Visual%20Representation%20Model.html#info","title":"\ud83d\udd25INFO","text":"<p>Blog: 2025/07/29 by IgniSavium</p> <ul> <li>Title: Reconstructing Visual Stimulus Images from EEG Signals Based on Deep Visual Representation Model</li> <li>Authors: Weijian Mai, Zhijun Zhang (South China University of Technology)</li> <li>Published: March 2024</li> <li>Comment: IEEE Transactions on Human-Machine Systems</li> <li>URL: https://ieeexplore.ieee.org/document/10683806</li> </ul> <p>\ud83e\udd5cTLDR: VAE framework to reconstruct digit imgs</p>"},{"location":"Reconstructing%20Visual%20Stimulus%20Images%20from%20EEG%20Signals%20Based%20on%20Deep%20Visual%20Representation%20Model.html#motivation","title":"Motivation","text":"<p>This paper aims to address the limitations of costly and less portable fMRI-based visual image (only consider numbers &amp; letters) reconstruction by proposing a novel EEG-based method that fundamentally learns deep visual representations\u2014unlike prior EEG approaches that relied heavily on generative models without effectively bridging EEG signals and visual semantics.</p>"},{"location":"Reconstructing%20Visual%20Stimulus%20Images%20from%20EEG%20Signals%20Based%20on%20Deep%20Visual%20Representation%20Model.html#model","title":"Model","text":""},{"location":"Reconstructing%20Visual%20Stimulus%20Images%20from%20EEG%20Signals%20Based%20on%20Deep%20Visual%20Representation%20Model.html#data","title":"Data","text":"<p>The EEG dataset was collected from four healthy subjects (two males and two females, aged 22\u201326) using a 32-channel EMOTIV EPOC Flex device at a 128\u202fHz sampling rate, during visual presentation of character images (numbers, uppercase, and lowercase letters) in three parts; each character type was shown for 100\u202fs with 1\u202fs image display and 1\u202fs idle intervals, producing 50 trials (i.e. fonts) per type, with preprocessing including 1\u201364\u202fHz bandpass filtering, baseline correction using the 1000\u202fms prior to stimulus onset, and discarding the first 100 time steps of each trial to reduce interference.</p>"},{"location":"Reconstructing%20Visual%20Stimulus%20Images%20from%20EEG%20Signals%20Based%20on%20Deep%20Visual%20Representation%20Model.html#architecture","title":"Architecture","text":"<p>VAE-like framework:</p> <p></p> <p></p> <p>\ud83e\uddd0It is intuitively equivalent with train a EEG Encoder to map EEG to Image VAE latent.</p>"},{"location":"Reconstructing%20Visual%20Stimulus%20Images%20from%20EEG%20Signals%20Based%20on%20Deep%20Visual%20Representation%20Model.html#evaluation","title":"Evaluation","text":"<p>We divided the EEG dataset (26 characters: A to Z; 26 characters: a to z; 10 characters: 0 to 9) three parts: training set, validation set, and test set, which are divided as 80%, 10%, and 10%, respectively.</p>"},{"location":"Reconstructing%20Visual%20Stimulus%20Images%20from%20EEG%20Signals%20Based%20on%20Deep%20Visual%20Representation%20Model.html#encoder-performance","title":"Encoder Performance","text":"<p>K Nearest Neighbor Classifier:</p> <p></p>"},{"location":"Reconstructing%20Visual%20Stimulus%20Images%20from%20EEG%20Signals%20Based%20on%20Deep%20Visual%20Representation%20Model.html#data-distribution-influence-on-image-reconstruction","title":"Data Distribution Influence on Image Reconstruction","text":"<p>The study demonstrates that the distribution of character type combinations in the training data significantly impacts the performance of EEG-based image reconstruction. Specifically, combinations involving the same character type (e.g., lowercase\u2013lowercase) yield better reconstruction results than mixed-type combinations. This is likely because characters of the same type share more consistent visual patterns, making it easier for the model to learn EEG-to-image mappings. Furthermore, the model also performs well on semantically meaningful character sets (e.g., \u201cBRAINS\u201d), indicating its ability to reconstruct recognizable images even with limited and specific input categories.</p>"},{"location":"Reconstructing%20Visual%20Stimulus%20Images%20from%20EEG%20Signals%20Based%20on%20Deep%20Visual%20Representation%20Model.html#qualitative-results","title":"Qualitative Results","text":"<p>\ud83e\uddd0Seemingly NOT style-sensitive enough (comparing different \"g\" styles)</p> <p>\ud83e\uddd0Maybe the training data is NOT style balanced (i.e. many kinds of font actually have overall similar styles)</p> <p></p>"},{"location":"Seeing%20through%20the%20Brain-Image%20Reconstruction%20of%20Visual%20Perception%20from%20Human%20Brain%20Signals.html","title":"Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals","text":""},{"location":"Seeing%20through%20the%20Brain-Image%20Reconstruction%20of%20Visual%20Perception%20from%20Human%20Brain%20Signals.html#info","title":"\ud83d\udd25INFO","text":"<p>Blog: 2025/07/22 by IgniSavium</p> <ul> <li>Title: Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals</li> <li>Authors: Yu-Ting Lan,Wei-Long Zheng  et al. (SJTU)</li> <li>Published: August 2023</li> <li>Comment: arxiv</li> <li>URL: https://arxiv.org/abs/2308.02510</li> </ul> <p>\ud83e\udd5cTLDR: EEG2img Reconstruction Using the Stable Diffusion Framework: Transforming EEG Signals into Fine-Granularity Image Silhouette Saliency Maps and Coarse-Granularity CLIP Text Embeddings for Descriptions.</p>"},{"location":"Seeing%20through%20the%20Brain-Image%20Reconstruction%20of%20Visual%20Perception%20from%20Human%20Brain%20Signals.html#motivation","title":"Motivation","text":"<p>This research addresses the challenge of reconstructing high-resolution visual stimuli from EEG signals, a task complicated by the temporal nature and noise of EEG data, and improves upon previous approaches by introducing a multi-level (actually 2-level) semantic decoding method that enhances image quality and semantic accuracy, filling the gap left by earlier methods that either struggled with low resolution or failed to capture semantic details.</p>"},{"location":"Seeing%20through%20the%20Brain-Image%20Reconstruction%20of%20Visual%20Perception%20from%20Human%20Brain%20Signals.html#model","title":"Model","text":""},{"location":"Seeing%20through%20the%20Brain-Image%20Reconstruction%20of%20Visual%20Perception%20from%20Human%20Brain%20Signals.html#pixel-level-gran","title":"Pixel-Level gran.","text":"<p>First, train a simple EEG feature extractor by clustering (a: anchor; p: positive ; n: negative):</p> <p></p> <p>Next, train a GAN saliency map generator (using $ f_\\theta $ above as input):</p> <p></p> <p>ms: mode seeking regularization ; SSIM: structural similarity index measure</p>"},{"location":"Seeing%20through%20the%20Brain-Image%20Reconstruction%20of%20Visual%20Perception%20from%20Human%20Brain%20Signals.html#sample-level-gran","title":"Sample-Level gran.","text":"<p>transform EEG signal to corresponding description CLIP text embeddings using L2 loss.</p>"},{"location":"Seeing%20through%20the%20Brain-Image%20Reconstruction%20of%20Visual%20Perception%20from%20Human%20Brain%20Signals.html#evaluation","title":"Evaluation","text":""},{"location":"Seeing%20through%20the%20Brain-Image%20Reconstruction%20of%20Visual%20Perception%20from%20Human%20Brain%20Signals.html#qualitative-results","title":"Qualitative Results","text":""},{"location":"Seeing%20through%20the%20Brain-Image%20Reconstruction%20of%20Visual%20Perception%20from%20Human%20Brain%20Signals.html#performance","title":"Performance","text":"<p>IS: Inception Score</p> <p></p>"},{"location":"Seeing%20through%20the%20Brain-Image%20Reconstruction%20of%20Visual%20Perception%20from%20Human%20Brain%20Signals.html#cross-subject-consistency","title":"Cross-Subject Consistency","text":"<p>\ud83e\udd14As having incorporated all-subject data in the training set, this Cross-Subject Consistency experiment is somehow pointless.</p> <p></p> <p></p>"},{"location":"Seeing%20through%20the%20Brain-Image%20Reconstruction%20of%20Visual%20Perception%20from%20Human%20Brain%20Signals.html#ablation","title":"Ablation","text":"<p>Insufficient visual structural information (saliency map in this case) has become a drag on for more informative captions (BLIP caption vs. simple label caption in this case).</p> <p></p> <p></p>"},{"location":"Seeing%20through%20the%20Brain-Image%20Reconstruction%20of%20Visual%20Perception%20from%20Human%20Brain%20Signals.html#reflections","title":"\ud83e\udd14Reflections","text":"<p>The feature extractor for EEG signals is poorly trained, resulting in GAN-generated saliency maps that are derived from ambiguous EEG features. These maps are highly inadequate for capturing the overall visual structure of the original image, including attributes such as position, size, and orientation.</p> <p>An alternative approach is to directly map EEG signals into the latent space of the DM (e.g., the latent states encoded from the original images) [already utilized by Yu Takagi and Shinji Nishimoto]. </p>"},{"location":"UniBrain-Unify%20Image%20Reconstruction%20and%20Captioning%20in%20One%20Diffusion%20Model%20from%20Brain%20Activity.html","title":"UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity","text":""},{"location":"UniBrain-Unify%20Image%20Reconstruction%20and%20Captioning%20in%20One%20Diffusion%20Model%20from%20Brain%20Activity.html#info","title":"\ud83d\udd25INFO","text":"<p>Blog: 2025/07/28 by IgniSavium</p> <ul> <li>Title: UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity</li> <li>Authors: Weijian Mai, Zhijun Zhang (South China University of Technology)</li> <li>Published: August 2023</li> <li>Comment: arxiv</li> <li>URL: https://arxiv.org/abs/2308.07428</li> </ul> <p>\ud83e\udd5cTLDR: Versatile Diffusion(VD) supporting both visual and textual input and output</p>"},{"location":"UniBrain-Unify%20Image%20Reconstruction%20and%20Captioning%20in%20One%20Diffusion%20Model%20from%20Brain%20Activity.html#motivation","title":"Motivation","text":"<p>This paper proposes UniBrain, a unified multi-modal diffusion model to overcome the limitations of prior fMRI-based methods\u2014such as separate pipelines and reliance on scarce training data\u2014by enabling simultaneous high-fidelity image reconstruction and captioning without model training or fine-tuning.</p>"},{"location":"UniBrain-Unify%20Image%20Reconstruction%20and%20Captioning%20in%20One%20Diffusion%20Model%20from%20Brain%20Activity.html#model","title":"Model","text":""},{"location":"UniBrain-Unify%20Image%20Reconstruction%20and%20Captioning%20in%20One%20Diffusion%20Model%20from%20Brain%20Activity.html#architecture","title":"Architecture","text":"<p>Original Versatile Diffusion(VD) architecture:</p> <p></p> <p>UniBrain based on Versatile Diffusion(VD) Framework:</p> <p></p> <p>It encodes visual stimuli and COCO captions into low- (\\(Z_I\\), \\(Z_T\\)) and high-level (\\(C_I\\), \\(C_T\\)) representations via frozen pretrained encoders(Consistent with original Versatile Diffusion model)\u2014\u2014Four small regressors map fMRI to these representations.</p> <p>During testing:</p> <ul> <li>Image reconstruction: \\(Z_I\\) is inferred and denoised via diffusion guided by \\(C_I\\) and \\(C_T\\), then decoded into an image by AutoKL Encoder.</li> <li>Captioning: \\(Z_T\\) is inferred and denoised via diffusion guided by \\(C_I\\) and \\(C_T\\), then decoded into text via Optimus GPT2, with repeated sentences removed.</li> </ul> <p>CLIP-Image and CLIP-Text jointly guide diffusion using a U-Net cross_attention matrix liner interpolation.</p>"},{"location":"UniBrain-Unify%20Image%20Reconstruction%20and%20Captioning%20in%20One%20Diffusion%20Model%20from%20Brain%20Activity.html#implementation","title":"Implementation","text":"<p>UniBrain uses Ridge regression to map fMRI to latent features, addressing multicollinearity. It runs 50 diffusion steps with strength 0.75 for both image and text reconstruction. The CLIP condition mixing rates are set to 0.6 for image reconstruction and 0.9 for captioning to optimize performance.</p>"},{"location":"UniBrain-Unify%20Image%20Reconstruction%20and%20Captioning%20in%20One%20Diffusion%20Model%20from%20Brain%20Activity.html#evaluation","title":"Evaluation","text":""},{"location":"UniBrain-Unify%20Image%20Reconstruction%20and%20Captioning%20in%20One%20Diffusion%20Model%20from%20Brain%20Activity.html#quantitative-results","title":"Quantitative Results","text":""},{"location":"UniBrain-Unify%20Image%20Reconstruction%20and%20Captioning%20in%20One%20Diffusion%20Model%20from%20Brain%20Activity.html#performance","title":"Performance","text":""},{"location":"UniBrain-Unify%20Image%20Reconstruction%20and%20Captioning%20in%20One%20Diffusion%20Model%20from%20Brain%20Activity.html#ablation","title":"Ablation","text":"<p>\\(Z_I\\) or \\(Z_T\\) (latent encoder) mainly captures low-level semantics (overall image or caption structure e.g. position orientation size) and \\(C_I\\) or \\(C_T\\) (CLIP encoder) mainly captures high-level semantics (e.g. category attribute)</p> <p>Note that, even when combined with \\(C_T\\) and \\(Z_I\\) , as with Takagi et al. [38], all quantitative high-level performance of the \u2018W/o \\(C_I\\) \u2019 model is still worse than the \u2018Only \\(C_I\\) \u2019 model. This phenomenon explains why UniBrain is superior to SD [38], since SD ignores the \\(C_I\\) features.</p> <p></p> <p></p> <p></p>"},{"location":"UniBrain-Unify%20Image%20Reconstruction%20and%20Captioning%20in%20One%20Diffusion%20Model%20from%20Brain%20Activity.html#synthetic-roi-specific-fmri-analysis","title":"Synthetic RoI-specific fMRI analysis","text":""},{"location":"UniBrain-Unify%20Image%20Reconstruction%20and%20Captioning%20in%20One%20Diffusion%20Model%20from%20Brain%20Activity.html#reflections","title":"\ud83e\uddd0Reflections","text":"<p>We should consider the Model Size comparison between SD and VD.</p> <p>This VD-based UniBrain framework emphasized the importance of \\(C_I\\) i.e. image clip features as backward diffusion conditions.</p>"},{"location":"Visual%20Decoding%20and%20Reconstruction%20via%20EEG%20Embeddings%20with%20Guided%20Diffusion.html","title":"Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion","text":""},{"location":"Visual%20Decoding%20and%20Reconstruction%20via%20EEG%20Embeddings%20with%20Guided%20Diffusion.html#info","title":"\ud83d\udd25INFO","text":"<p>Blog: 2025/07/30 by IgniSavium</p> <ul> <li>Title: Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion</li> <li>Authors: Dongyang Li et al. (Southern University of Science and Technology)</li> <li>Published: March 2024</li> <li>Comment: NIPS2024</li> <li>URL: https://arxiv.org/abs/2403.07721</li> </ul> <p>\ud83e\udd5cTLDR: diffusion prior + SDXL-Turbo + IP-Adapter</p>"},{"location":"Visual%20Decoding%20and%20Reconstruction%20via%20EEG%20Embeddings%20with%20Guided%20Diffusion.html#motivation","title":"Motivation","text":"<p>This paper aims to address the limited practicality and performance gap of EEG-based visual decoding compared to fMRI-based methods by introducing a novel zero-shot EEG-to-image reconstruction framework that achieves state-of-the-art results through a tailored encoder and two-stage generation (prior diffusion to convert \\(Z_{EEG}\\) to \\(Z_{img}\\)) strategy, overcoming previous challenges like low signal quality and limited model design.</p>"},{"location":"Visual%20Decoding%20and%20Reconstruction%20via%20EEG%20Embeddings%20with%20Guided%20Diffusion.html#model","title":"Model","text":""},{"location":"Visual%20Decoding%20and%20Reconstruction%20via%20EEG%20Embeddings%20with%20Guided%20Diffusion.html#architecture","title":"Architecture","text":"<p>EEG Encoder (Channel-wise Transformer + Synthesis Module):</p> <p></p> <p>Image Reconstruction training uses weighted loss of these two:</p> <ol> <li>contrastive loss between EEG-Encoder and CLIP Visual Encoder (\\(R^{1024}\\) in ViT-14/Large)</li> <li>MSE to encoder EEG into image's VAE latent</li> </ol> <p>Two-stage EEG-guided image generation pipeline using conditional diffusion models (ref. DALLE-2):</p> <ol> <li>Stage I \u2013 EEG-Conditioned Diffusion:    EEG embeddings condition a diffusion model to generate CLIP image embeddings using a U-Net and classifier-free guidance.</li> </ol> <p></p> <ol> <li> <p>Stage II \u2013 Image Synthesis:    The generated CLIP embedding guides image generation using pre-trained SDXL(Stable Diffusion) and IP-Adapter    (ImagePrompt: adapter weights for image-embedding-conditioned SD), with optional acceleration via SDXL-Turbo.</p> </li> <li> <p>Low-Level Pipeline:    To recover fine visual details (e.g., contours, posture), EEG featurevs are aligned with VAE latents. Only latent-space loss works reliably; full image-level losses are unstable and memory-intensive.</p> </li> <li> <p>Semantic-Level Pipeline:    EEG-derived image features generate captions via GIT, optionally guiding generation via text prompts. Due to potential semantic drift, this step is not always applied.</p> </li> </ol>"},{"location":"Visual%20Decoding%20and%20Reconstruction%20via%20EEG%20Embeddings%20with%20Guided%20Diffusion.html#evaluation","title":"Evaluation","text":""},{"location":"Visual%20Decoding%20and%20Reconstruction%20via%20EEG%20Embeddings%20with%20Guided%20Diffusion.html#overall-performance","title":"Overall Performance","text":""},{"location":"Visual%20Decoding%20and%20Reconstruction%20via%20EEG%20Embeddings%20with%20Guided%20Diffusion.html#reconstruction-performance","title":"Reconstruction Performance","text":""},{"location":"Visual%20Decoding%20and%20Reconstruction%20via%20EEG%20Embeddings%20with%20Guided%20Diffusion.html#temporal-and-spatial-interpretability","title":"Temporal and Spatial Interpretability","text":""},{"location":"Visual%20Decoding%20and%20Reconstruction%20via%20EEG%20Embeddings%20with%20Guided%20Diffusion.html#retrieval-and-classification-performance","title":"Retrieval and Classification Performance","text":""},{"location":"Where%20Does%20EEG%20Come%20From%20and%20What%20Does%20It%20Mean.html","title":"Where Does EEG Come From and What Does It Mean?","text":""},{"location":"Where%20Does%20EEG%20Come%20From%20and%20What%20Does%20It%20Mean.html#info","title":"\ud83d\udd25INFO","text":"<p>Blog: 2025/07/18 by IgniSavium</p> <ul> <li>Title: Where Does EEG Come From and What Does It Mean?</li> <li>Authors: Michael X Cohen (Radboud University)</li> <li>Published: April 2017</li> <li>Comment: Trends in Neurosciences</li> <li>URL: https://www.cell.com/trends/neurosciences/abstract/S0166-2236(17)30024-3?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0166223617300243%3Fshowall%3Dtrue</li> </ul> <p>\ud83e\udd5cTLDR: A comprehensive introduction about EEG and its related studies (more from a physiological perspective). We mainly study the typical wave forms or patterns in EEG.</p>"},{"location":"Where%20Does%20EEG%20Come%20From%20and%20What%20Does%20It%20Mean.html#motivation","title":"Motivation","text":"<p>This paper emphasizes that the real, often overlooked question is not merely where EEG signals originate anatomically, but what they truly represent in terms of underlying neural circuit dynamics\u2014arguing that we still understand surprisingly little about this relationship and highlighting the need to bridge this gap using modern neuroscience tools and a focus on interpretable EEG features like neural oscillations.</p> <p><code>cognitive process</code> &lt;-<code>neural circuit dynamics</code> -&gt; <code>EEG signals</code></p> <p>\"Therefore, the right question is: what are the neural microcircuit functional/anatomical configurations \u2013 the dynamics within and among different classes of cells, different layers within the cortex, and different columns across the cortical sheet \u2013 that produce the various spatial/spectral/temporal EEG features that have been linked to cognitive processes?\"</p> <p>EEG feature: this term is used here as shorthand for an idiosyncratic spatial/temporal/spectral pattern that is associated with a particular sensory or cognitive process, similar to a \u2018fingerprint\u2019. Examples include midfrontal theta and response conflict monitoring, and posterior alpha power and spatial attention.</p> <p></p>"},{"location":"Where%20Does%20EEG%20Come%20From%20and%20What%20Does%20It%20Mean.html#eeg-itself","title":"EEG itself","text":"<p>Electroencephalography (EEG): the measurement of brain electrical fields via electrodes (which act as small antennas) placed on the head (non-invasive). The electrical fields are the result of electrochemical signals passing from one neuron to the next. When billions of these tiny signals are passed simultaneously in spatially extended and geometrically aligned neural populations, the electrical fields sum and become powerful enough to be measured from outside the head.</p> <p>Perhaps the main disadvantage is that EEG is limited to large, synchronous populations of neurons; small-scale and asynchronous activity is difficult or impossible to measure.</p> <p>The origins of the local field potential (LFP) and EEG: the (here termed) \u2018standard model\u2019 states that LFP and EEG are the extracellular currents reflecting summed dendritic postsynaptic potentials (the exchange of electrochemical signaling across the synapse) in thousands to millions of pyramidal cells in parallel alignment.</p>"},{"location":"Where%20Does%20EEG%20Come%20From%20and%20What%20Does%20It%20Mean.html#eeg-patterns","title":"EEG patterns","text":""},{"location":"Where%20Does%20EEG%20Come%20From%20and%20What%20Does%20It%20Mean.html#two-representative-waves","title":"Two representative waves","text":""},{"location":"Where%20Does%20EEG%20Come%20From%20and%20What%20Does%20It%20Mean.html#alpha-waves-812-hz","title":"Alpha Waves (8\u201312 Hz)","text":"<ul> <li>What it is: A type of brain wave commonly observed when a person is relaxed, eyes closed, but still awake. It reflects the brain's mechanism for suppressing irrelevant information while maintaining rhythmic attentional control.</li> <li>Key Identification Features: Stable frequency, strong energy, and clearly visible on EEG charts.</li> <li>Computer Recognition Methods:</li> <li>Can extract signals in the 8\u201312 Hz range using Fast Fourier Transform (FFT)\uff08\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\uff09 or Power Spectral Density (PSD) analysis\uff08\u529f\u7387\u8c31\u5bc6\u5ea6\u5206\u6790\uff09.</li> <li>Simple spectral analysis tools\uff08\u9891\u8c31\u5206\u6790\u5de5\u5177\uff09 can accurately detect these waves.</li> <li>Very easy to observe during eye-closed or resting states; one of the easiest types of brain waves to identify.</li> </ul>"},{"location":"Where%20Does%20EEG%20Come%20From%20and%20What%20Does%20It%20Mean.html#gamma-waves-3080-hz","title":"Gamma Waves (30\u201380 Hz)","text":"<ul> <li>What it is: High-speed brain waves that occur during active perception, focused attention, and information integration. They may help coordinate the synchronized activity of neural populations.</li> <li>Identification Challenges:</li> <li>High frequency with short duration;</li> <li>Easily masked by electromyographic noise or external interference;</li> <li>Low signal-to-noise ratio in standard EEG equipment, making them more challenging to identify.</li> <li>Computer Recognition Methods:</li> <li>Use time-frequency analysis methods like Wavelet Transform\uff08\u5c0f\u6ce2\u53d8\u6362\uff09 or Short-Time Fourier Transform (STFT)\uff08\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362\uff09 to capture transient high-frequency components.</li> <li>Apply blind source separation techniques such as Independent Component Analysis (ICA)\uff08\u72ec\u7acb\u6210\u5206\u5206\u6790\uff09 to remove artifacts and noise.</li> <li>Multi-channel spatial information analysis, such as Common Spatial Patterns (CSP)\uff08\u5171\u7a7a\u95f4\u6a21\u5f0f\uff09 and signal envelope analysis\uff08\u4fe1\u53f7\u5305\u7edc\u5206\u6790\uff09, may be required to improve recognition accuracy.</li> </ul> EEG Waveform Functional Role Recognition Difficulty Common Computational Methods Challenges &amp; Advantages Alpha Noise suppression, attention maintenance \u2705 Easy to recognize FFT, PSD, Band-pass filtering Clear waveform, stable frequency Gamma Sensory processing, synchronization \u26a0\ufe0f More difficult to recognize Wavelet transform, ICA, STFT, CSP, etc. High-frequency noise, requires advanced denoising and signal enhancement"},{"location":"Where%20Does%20EEG%20Come%20From%20and%20What%20Does%20It%20Mean.html#general-easily-recognizable-patterns","title":"General Easily-Recognizable Patterns","text":"<ol> <li>Classic Band Oscillations \u7ecf\u5178\u5e26\u72b6\u632f\u8361 </li> <li>\u03b1 Waves (8\u201313 Hz): Large amplitude, prominent in the occipital region during eyes-closed rest, easy to identify.  </li> <li>\u03b2 Waves (13\u201330 Hz): High frequency, linked to alertness and attention, stable recognition.  </li> <li>\u03b8 Waves (4\u20138 Hz): Clear waveform during specific tasks or sleep stages.  </li> <li> <p>\u03b4 Waves (0.5\u20134 Hz): High amplitude, slow cycles in deep sleep, easily detectable.  </p> </li> <li> <p>Event-Related Potentials (ERPs) \u4e8b\u4ef6\u76f8\u5173\u7535\u4f4d </p> </li> <li> <p>Time-locked waveforms like P300, N400, with high clarity and repeatability after averaging trials.  </p> </li> <li> <p>Transient Bursts \u77ac\u6001\u7206\u53d1 </p> </li> <li> <p>Brief but noticeable enhancements of \u03b1 or \u03b8 waves, easily captured via time-frequency analysis.  </p> </li> <li> <p>Non-Sinusoidal Patterns (e.g., Up-Down States) \u975e\u6b63\u5f26\u6a21\u5f0f </p> </li> <li> <p>Periodic, large-amplitude asymmetric waves during anesthesia or deep sleep, visually distinguishable.  </p> </li> <li> <p>Spatially Distinct Patterns \u7a7a\u95f4\u72ec\u7279\u6a21\u5f0f </p> </li> <li>Stable spatial distribution of \u03b1 waves in the occipital region under high-density EEG, easy to localize.  </li> </ol>"},{"location":"%5BSurvey%5DDeep%20Learning%20for%20EEG-Based%20Visual%20Classification%20and%20Reconstruction%5BSEU%5D.html","title":"Deep Learning for EEG-Based Visual Classification and Reconstruction: Panorama, Trends, Challenges and Opportunities","text":""},{"location":"%5BSurvey%5DDeep%20Learning%20for%20EEG-Based%20Visual%20Classification%20and%20Reconstruction%5BSEU%5D.html#info","title":"\ud83d\udd25INFO","text":"<p>Blog: 2025/07/17 by IgniSavium</p> <ul> <li>Title: Deep Learning for EEG-Based Visual Classification and Reconstruction: Panorama, Trends, Challenges and Opportunities</li> <li>Authors: Wei Li, Penglu Zhao, Cheng Xu, Yingting Hou, and Aiguo Song ( Southeast University)</li> <li>Published: May 2025</li> <li>Comment: IEEE Transactions on Biomedical Engineering</li> <li>URL: https://ieeexplore.ieee.org/document/10993346</li> </ul> <p>\ud83e\udd5cTLDR: This paper provides the first review on EEG-based visual classification and reconstruction, comprehensively summarizing the representative deep learning methods from both feature encoding and decoding perspectives.</p>"},{"location":"%5BSurvey%5DDeep%20Learning%20for%20EEG-Based%20Visual%20Classification%20and%20Reconstruction%5BSEU%5D.html#comparison-with-previous-surveys","title":"Comparison with previous surveys","text":"<p>This review highlights EEG-based visual classification and reconstruction using deep learning, distinguishing itself from prior works in several ways:</p> <ul> <li>Robinson et al. focus on theoretical neural decoding, especially cognitive processes, but do not cover practical applications like image reconstruction. </li> <li>Wang et al. center on GANs for EEG signal processing tasks like data augmentation and artifact removal (method-oriented perspective). </li> <li>Rakhimberdina et al. explore deep learning for fMRI-based image reconstruction. However, EEG's high temporal resolution and low spatial fidelity pose different challenges.</li> <li>Huang et al. focus on fMRI-based visual decoding, whereas EEG faces more severe domain gap issues, which are a central concern of this review due to its temporal resolution and data variability.</li> <li>Nestor et al. discuss face image reconstruction from fMRI and behavioral data.</li> </ul> <p>Overall, this review fills gaps by addressing EEG-specific challenges and offering a broader, task-focused perspective compared to existing surveys.</p>"},{"location":"%5BSurvey%5DDeep%20Learning%20for%20EEG-Based%20Visual%20Classification%20and%20Reconstruction%5BSEU%5D.html#methods-summary","title":"Methods Summary","text":"<p>Generally utilize an Encoder-Decoder architecture.</p> <p></p> <p></p>"}]}